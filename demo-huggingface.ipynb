{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae35d6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fsspec\n",
    "import os\n",
    "from ipfsspec.asyn import AsyncIPFSFileSystem\n",
    "from fsspec import register_implementation\n",
    "import asyncio\n",
    "import io\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "# register_implementation(IPFSFileSystem.protocol, IPFSFileSystem)\n",
    "# register_implementation(AsyncIPFSFileSystem.protocol, AsyncIPFSFileSystem)\n",
    "\n",
    "# with fsspec.open(\"ipfs://QmZ4tDuvesekSs4qM5ZBKpXiZGun7S2CYtEZRB3DYXkjGx\", \"r\") as f:\n",
    "#     print(f.read())\n",
    "class fs:\n",
    "    ipfs = AsyncIPFSFileSystem()\n",
    "    local = fsspec.filesystem(\"file\")\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "class deML:\n",
    "    tmp_root_path = '/tmp/deML'\n",
    "    fs = fs\n",
    "\n",
    "    @staticmethod\n",
    "    def get_tmp_path(path):\n",
    "        tmp_path = os.path.join(deML.tmp_root_path, path)\n",
    "        try:\n",
    "            fs.local.mkdir(tmp_path, create_parents=True)\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "        \n",
    "        return tmp_path\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def save_model(model, path:str):\n",
    "\n",
    "        \n",
    "        # fs.ipfs.mkdir(path, create_parents=True)\n",
    "        \n",
    "        tmp_path = deML.get_tmp_path(path=path)\n",
    "        model.save_pretrained(tmp_path)\n",
    "        fs.ipfs.mkdirs(path)\n",
    "        \n",
    "        cid = deML.ipfs_put(lpath=tmp_path, rpath=path, max_trials=10)\n",
    "        fs.local.rm(tmp_path,  recursive=True)\n",
    "        \n",
    "        return cid\n",
    "\n",
    "    @staticmethod\n",
    "    def save_tokenizer(tokenizer, path:str):\n",
    "\n",
    "        \n",
    "        # fs.ipfs.mkdir(path, create_parents=True)\n",
    "        \n",
    "        tmp_path = deML.get_tmp_path(path=path)\n",
    "        tokenizer.save_pretrained(tmp_path)\n",
    "        fs.ipfs.mkdirs(path)\n",
    "        \n",
    "        cid = deML.ipfs_put(lpath=tmp_path, rpath=path, max_trials=10)\n",
    "        fs.local.rm(tmp_path,  recursive=True)\n",
    "        \n",
    "        return cid\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_tokenizer( path:str):\n",
    "        tmp_path = deML.get_tmp_path(path=path)\n",
    "        fs.ipfs.get(lpath=tmp_path, rpath=path )\n",
    "        model = AutoTokenizer.from_pretrained(tmp_path)\n",
    "        fs.local.rm(tmp_path,  recursive=True)\n",
    "        return model\n",
    "\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_model( path:str):\n",
    "        tmp_path = deML.get_tmp_path(path=path)\n",
    "        fs.ipfs.get(lpath=tmp_path, rpath=path )\n",
    "        model = AutoModel.from_pretrained(tmp_path)\n",
    "        # fs.local.rm(tmp_path,  recursive=True)\n",
    "        return model\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def load_dataset(path):\n",
    "        tmp_path = deML.get_tmp_path(path=path)\n",
    "        fs.ipfs.get(lpath=tmp_path, rpath=path )\n",
    "        dataset = Dataset.load_from_disk(tmp_path)\n",
    "        # fs.local.rm(tmp_path,  recursive=True)\n",
    "        \n",
    "        return dataset\n",
    "\n",
    "    @staticmethod\n",
    "    def save_dataset(dataset, path:str):\n",
    "        tmp_path = deML.get_tmp_path(path=path)\n",
    "        dataset = dataset.save_to_disk(tmp_path)\n",
    "        cid = deML.ipfs_put(lpath=tmp_path, rpath=path, max_trials=10)\n",
    "        # fs.local.rm(tmp_path,  recursive=True)\n",
    "        return cid\n",
    "\n",
    "\n",
    "    \n",
    "          \n",
    "    @staticmethod\n",
    "    def ipfs_put(lpath, rpath, max_trials=10):\n",
    "        trial_count = 0\n",
    "        cid = None\n",
    "        while trial_count<max_trials:\n",
    "            try:\n",
    "                cid= fs.ipfs.put(lpath=lpath, rpath=rpath, recursive=True)\n",
    "                break\n",
    "            except fsspec.exceptions.FSTimeoutError:\n",
    "                trial_count += 1\n",
    "                print(f'Failed {trial_count}/{max_trials}')\n",
    "                \n",
    "        return cid\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc008528",
   "metadata": {},
   "source": [
    "## Loading Model from Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8791b11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-18 12:37:25.132 WARNING datasets.builder: Reusing dataset glue (/root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "dataset, model, tokenizer = {}, {}, {}\n",
    "dataset['web2'] = load_dataset(\"glue\", \"mrpc\", split=\"train\")\n",
    "model['web2'] = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer['web2'] = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "134d9e8c-0141-4679-a14a-787505a96b2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokenizer': 'QmNcERjBC4TC4HhYeRRXaoq6xF6s5sExFvF9jjCKzt68z5',\n",
       " 'model': 'QmY2TSXiDckZ9kuqsGpFKyKz5bVZcm61FN77M8nfbxfzAh',\n",
       " 'dataset': 'QmeGhwGt5X3KvuVrx1orfcCBAbwG4aJZn9n5yBpWkbPu5w'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "cid = {}\n",
    "cid['tokenizer'] = deML.save_tokenizer(tokenizer=tokenizer['web2'], path='/hf_tokenizer')\n",
    "cid['model'] = deML.save_model(model=model['web2'], path='/hf_model')\n",
    "cid['dataset'] = deML.save_dataset(dataset=dataset['web2'], path='/hf_dataset')\n",
    "cid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f986170e-dd15-48e4-94e5-c88ba64abe67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model['web3'] = deML.load_model(cid['model'])\n",
    "tokenizer['web3']  = deML.load_tokenizer(cid['tokenizer'])\n",
    "dataset['web3']  = deML.load_dataset(cid['dataset'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "266bb603-d848-41e1-8990-37cce8372e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-18 12:38:42.130 WARNING datasets.arrow_dataset: Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-94e25bed60176d8e.arrow\n",
      "2022-08-18 12:38:42.172 WARNING datasets.arrow_dataset: Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-e5a82a27b6ac415d.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING WEB2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-18 12:38:45.655 WARNING datasets.arrow_dataset: Loading cached processed dataset at /tmp/deML/QmeGhwGt5X3KvuVrx1orfcCBAbwG4aJZn9n5yBpWkbPu5w/cache-d0ec264864b2f8e5.arrow\n",
      "2022-08-18 12:38:45.698 WARNING datasets.arrow_dataset: Loading cached processed dataset at /tmp/deML/QmeGhwGt5X3KvuVrx1orfcCBAbwG4aJZn9n5yBpWkbPu5w/cache-d3ba5c4f1220803e.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING WEB3\n"
     ]
    }
   ],
   "source": [
    "def run_inference(model, tokenizer, dataset):\n",
    "    def encode(examples):\n",
    "        return tokenizer(examples[\"sentence1\"], examples[\"sentence2\"], truncation=True, padding=\"max_length\")\n",
    "    dataset = dataset.map(encode, batched=True)\n",
    "    dataset = dataset.map(lambda examples: {\"labels\": examples[\"label\"]}, batched=True)\n",
    "\n",
    "\n",
    "    dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\", \"labels\"])\n",
    "    import torch\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=1)\n",
    "\n",
    "    x_batch = next(iter(dataloader))\n",
    "    del x_batch['labels']\n",
    "    with torch.no_grad():\n",
    "        output_dict = dict(model(**x_batch))\n",
    "    return output_dict\n",
    "    \n",
    "results = {}\n",
    "print('RUNNING WEB2')\n",
    "results['web2'] = run_inference(model['web2'], tokenizer['web2'], dataset['web2'])\n",
    "# results['web3'] = run_inference(model['web3'], tokenizer['web3'], dataset['web3'])\n",
    "\n",
    "print('RUNNING WEB3')\n",
    "results['web3'] = run_inference(model['web3'], tokenizer['web3'], dataset['web3'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9fa6039c-6708-4977-afd1-56b87e17705b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'last_hidden_state': {'web2': tensor(-0.0085), 'web3': tensor(-0.0085)},\n",
       " 'pooler_output': {'web2': tensor(-0.0395), 'web3': tensor(-0.0395)}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "{k: {'web2': results['web2'][k].mean(), 'web3': results['web3'][k].mean()}for k in results['web3'].keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf7d03d-bcb8-4db0-9c21-fed23c645063",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f74a233-80cd-4b2a-812d-5363062a94e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
